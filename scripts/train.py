import sys, pathlib
sys.path.insert(0, str(pathlib.Path(__file__).resolve().parents[1]))

#!/usr/bin/env python
"""
train.py  -  Step 5.3
"""

from pathlib import Path
import torch
from torch.utils.data import DataLoader
import torch.nn.functional as F
from torchvision.datasets import MNIST

from src.data_utils import get_train_val_datasets, BASE_TRANSFORM
from src.models import LeNet5

from torch.optim.lr_scheduler import CosineAnnealingLR


ROOT = Path("data")
NUM_EPOCHS = 20


def accuracy(logits, labels):
    return (logits.argmax(1) == labels).float().mean()

def main():
    train_ds, val_ds = get_train_val_datasets(ROOT, use_aug=True)
    train_dl = DataLoader(train_ds, batch_size=128, shuffle=True)
    val_dl   = DataLoader(val_ds,   batch_size=256)

    model = LeNet5()
    opt   = torch.optim.Adam(model.parameters(), lr=1e-3)
    scheduler = CosineAnnealingLR(opt, T_max=NUM_EPOCHS, eta_min=1e-5)
    best_acc = 0.0

    for epoch in range(1, NUM_EPOCHS + 1):
        model.train()
        running_loss = 0.0
        for x, y in train_dl:
            opt.zero_grad()
            logits = model(x)
            loss = F.cross_entropy(logits, y)
            loss.backward()
            opt.step()
            running_loss += loss.item() * x.size(0)

        avg_loss = running_loss / len(train_ds)
        
        model.eval()
        correct = total = 0
        with torch.no_grad():
            for x, y in val_dl:
                logits = model(x)
                correct += (logits.argmax(1) == y).sum().item()
                total   += y.size(0)
        val_acc = correct / total

        scheduler.step()

        print(f"Epoch {epoch:02}/{NUM_EPOCHS}  "
            f"loss: {avg_loss:.4f}  "
            f"val_acc: {val_acc:.3%}")
        
        if val_acc > best_acc:
            best_acc = val_acc
            torch.save(model.state_dict(), "models/lenet_mnist.pt")
            print(f"  âœ” saved new best model ({best_acc:.3%})")

if __name__ == "__main__":
    main()
